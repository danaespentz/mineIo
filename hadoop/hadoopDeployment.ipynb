{
  "metadata": {
    "name": "Εξαμηνιαίο Project στα Προχωρημένα Θέματα Βάσεων Δεδομένων",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1rFd1sVa-DG"
      },
      "source": [
        "### 0. Preliminary - Setting up the working environment of Virtual Machines on the okeanos-knossos platform\n",
        "\n",
        "Among the prerequisites for the semester's assignment is the creation of Virtual Machines (VMs) and configuring the working environment using the Apache Hadoop and Apache Spark frameworks. Below is a detailed guide for the students regarding the configuration of this particular environment.\n",
        "\n",
        "#### 0.1 Creating an account on the cloud service *~okeanos-knossos*\n",
        "To *~okeanos-knossos* is an Infrastructure-as-a-Service (IAAS) that allows users to create and manage virtual computing systems. In recent years, similar services like Amazon AWS, Google Cloud, and Microsoft Azure have dominated the market because they provide customers with access to computational resources without the burden of maintaining the hardware infrastructure cost. In contrast to these commercial services, *~okeanos-knossos* has been developed for research purposes and is provided to students for free.\n",
        "\n",
        "As part of this assignment, students are initially requested to create an account on the *~okeanos-knossos* service by following the link:  https://okeanos-knossos.grnet.gr/home/.\n",
        "\n",
        "By choosing registration using their academic account at the National Technical University of Athens (NTUA), students can complete the creation of their academic accounts on this service.\n",
        "\n",
        "\n",
        "#### 0.2 Enrollment in the course project\n",
        "Next, in order to allocate the necessary computing resources, students must enroll in the project corresponding to the **Advanced Topics in Databases course**. Specifically, by following the link: https://astakos.okeanos-knossos.grnet.gr/ui/projects/search, students should search for the *project* `advancedDB.dblab.ntua.gr` and create a registration request.\n",
        "\n",
        "#### 0.3 Creating a pair of cryptographic keys and registering it with the *~okeanos-knossos* service\n",
        "Before providing information regarding the creation of Virtual Machines, it is advisable to ensure the existence of a pair of cryptographic keys on the computer from which users will have remote access to their resources in the *~okeanos-knossos* service. To create a new pair of cryptographic keys, it is sufficient to execute the following command (in a Linux environment) and follow the instructions:`$ ssh-keygen`.\n",
        "\n",
        "- Alternatively, a complete guide on how to create a key pair is available at the link: https://www.digitalocean.com/community/tutorials/how-to-set-up-ssh-keys-on-ubuntu-20-04.\n",
        "\n",
        "Upon completing the process, the contents of the public key will be displayed on the screen with the command:\n",
        "\n",
        "`$ cat ~/.ssh/id_rsa`.\n",
        "\n",
        "Next, you need to register the public key in your ~okeanos-knossos account. By following the link https://cyclades.okeanos-knossos.grnet.gr/ui/#public-keys/ and choosing to create a new key pair, users can import their key and associate it with their account using a chosen name.\n",
        "\n",
        "#### 0.4 Virtual Machine Creation\n",
        "The next step is the creation of the Virtual Machines (VMs), where the software infrastructure will be installed. By following the link https://cyclades.okeanos-knossos.grnet.gr/ui/#machines/icon/, users will have the ability to create new VMs. You are encouraged to create 2 VMs with the following characteristics:\n",
        " - Ubuntu Server LTS 16.04 OS\n",
        " - 4 CPUs\n",
        " - 8GB RAM\n",
        " - 30GB disk capacity\n",
        "\n",
        "After initiating the creation of a new VM, students should make the above selections in the following menus.\n",
        "\n",
        "**Caution!**\n",
        " - It is important, during the process of creating the Virtual Machines, to import the public key generated in the previous step; otherwise, access to the VMs will not be possible.\n",
        " - After the VMs are created, it's crucial to store the automatically generated system password for the user because, in case of loss, the process will need to be repeated.\n",
        "\n",
        "\n",
        "Once the virtual machines are successfully created, users can access them with the command:\n",
        "```\n",
        "$ ssh user@snf-****-ok-kno.grnetcloud.net\n",
        "```\n",
        "\n",
        "Since this is the first time remote connection to the specific system is requested, the local operating system will prompt for confirmation of the action:\n",
        "```\n",
        "Are you sure you want to continue connecting (yes/no/[fingerprint])? yes\n",
        "```\n",
        "\n",
        "Subsequently, it is recommended to change the automatically generated password using the command:\n",
        "```\n",
        "$ passwd\n",
        "```\n",
        "\n",
        "The same steps should be followed for creating the second VM.\n",
        "\n",
        "#### 0.5 Configuration of the public and private network\n",
        "The *~okeanos-knossos* service provides us with a unique public IPv4 address. Network resource management can be accessed through the link: https://cyclades.okeanos-knossos.grnet.gr/ui/#networks/. It is recommended for students to assign this specific IP to one of their VMs (the master). Subsequently, in the following steps, the public IP will need to be disconnected from the specified VM and connected to the other one to successfully complete the operating system upgrade process (see 1.2 below).\n",
        "\n",
        "- **Be very cautious with the use of the public IP. You can use a firewall, disconnect the IP when not in use, or even disable your virtual machines entirely. In the event of an attack, there is a possibility that your IP may be automatically classified as vulnerable, and you could lose access to the IPv4 address space and, of course, access to your cluster's web applications.**\n",
        "\n",
        "Furthermore, for your convenience, it is recommended to change the hostnames of the VMs. This is executed on the master and the worker, respectively:\n",
        "\n",
        "```bash\n",
        "$ sudo hostnamectl set-hostname okeanos-master\n",
        "$ sudo hostnamectl set-hostname okeanos-worker\n",
        "```\n",
        "\n",
        "Next is the configuration of the private network through which the two VMs will communicate. By following the link: https://cyclades.okeanos-knossos.grnet.gr/ui/#networks/, users should create a private IPv4 network and assign an IP address from this network to each VM.\n",
        "\n",
        "Afterwards, you should add the IP of your private network to both machines with the corresponding name in the /etc/hosts file. The final image of this specific file should be as follows for both nodes:\n",
        "\n",
        "```text\n",
        "127.0.0.1       localhost\n",
        "192.168.0.xxx   okeanos-master\n",
        "192.168.0.xxx   okeanos-worker\n",
        "\n",
        "\n",
        "# The following lines are desirable for IPv6 capable hosts\n",
        "::1             localhost ip6-localhost ip6-loopback\n",
        "ff02::1         ip6-allnodes\n",
        "ff02::2         ip6-allrouters\n",
        "```\n",
        "Next we need to restart the VMs with the command:\n",
        "```\n",
        "sudo reboot\n",
        "```\n",
        "After the VMs restart, we connect to the master and create a new cryptographic key that will be used for this node to access both itself and the worker through SSH. Similar to the previous steps (see step 0.3):\n",
        "```bash\n",
        "$ ssh-keygen -t rsa #press [ENTER] as many times as promted\n",
        "$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
        "```\n",
        "We print the contents of the key in the terminal with the following command and then insert it into the worker VM:\n",
        "```bash\n",
        "$ cat ~/.ssh/id_rsa.pub #στο master\n",
        "$ vim ~/.ssh/authorized_keys #στο worker: copy above output and paste it here\n",
        "```\n",
        "We confirm SSH access from the master to both VMs.\n",
        "```bash\n",
        "$ ssh okeanos-master\n",
        "$ ssh okeanos-worker\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0jwIL1Ba-DU"
      },
      "source": [
        "### 1. Optional (**RECOMMENDED!**): Updating and upgrading the VMs' operating system.\n",
        "\n",
        "We recommend updating and upgrading the software on your worker nodes to ensure full compatibility with the latest versions of all the software you may want to use.\n",
        "#### 1.1 Ubuntu 16.04 -> Ubuntu 18.04\n",
        "First, connect to your VM:\n",
        "```bash\n",
        "$ ssh user@snf-****-ok-kno.grnetcloud.net\n",
        "```\n",
        "Next, update + upgrade your existing OS software:\n",
        "```bash\n",
        "$ sudo apt update && sudo apt upgrade -y\n",
        "```\n",
        "Enter the password whenever prompted and accept all changes. When asked about installing new versions of configuration files, please select '`Install the package maintainer's version`' (unlike what you see in the picture below :D ).\n",
        "![Select the 'Install the package maintainer's version' option](https://onedrive.live.com/embed?resid=23805F5DB37ABB76%212098&authkey=%21AAcRhT7t6gVEXKY&width=1194&height=336)\n",
        "\n",
        "Restart the VM:\n",
        "```bash\n",
        "$ sudo reboot\n",
        "```\n",
        "After giving it a few seconds to reboot, reconnect (via SSH) and enter the following command to initiate the version upgrade:\n",
        "```bash\n",
        "$ sudo do-release-upgrade\n",
        "```\n",
        "Following the instructions, we accept the upgrade initially by choosing `'y'`. The next prompt asks us to press `[ENTER]`, and then another `'y'` to proceed with the process. The next dialogue prompts us to select our keyboard configuration, and then we are asked again if we want to keep previous system settings. We should once again select `Install the package maintainer's version`.\n",
        "\n",
        "![Select the 'Install the package maintainer's version' option](https://onedrive.live.com/embed?resid=23805F5DB37ABB76%212099&authkey=%21ALpqz7AEdZ1O7Gk&width=483&height=198)\n",
        "\n",
        "Next, a dialogue will appear asking us to select a system disk for the installation of the GRUB (boot loader). We should choose `/dev/vda` by pressing `[SPACE]` and then `[ENTER]`.\n",
        "\n",
        "**Caution**: In case of having chosen the wrong option at this step, the process will need to be repeated from the start, as it will cause your VMs to not boot!\n",
        "\n",
        "PS. Of course, anyone interested can find more information about GRUB on the internet (https://en.wikipedia.org/wiki/GNU_GRUB).\n",
        "![Pick '/dev/vda'](https://onedrive.live.com/embed?resid=23805F5DB37ABB76%212100&authkey=%21AH0yW9WfYKfCWRA&width=1251&height=378)\n",
        "\n",
        "As before, we are asked once again if we want to keep previous system settings. Once again, we should select 'Install the package maintainer's version'.\n",
        "\n",
        "![Select the 'Install the package maintainer's version' option](https://onedrive.live.com/embed?resid=23805F5DB37ABB76%212101&authkey=%21AMO4abNJt-MmMLs&width=482&height=198)\n",
        "\n",
        "Finally, another prompt will prompt us to remove installed packages that are no longer necessary. After accepting it, the system will prompt us to proceed with a restart and will start loading the latest version of the operating system.\n",
        "\n",
        "#### 1.2 Ubuntu 18.04 -> Ubuntu 20.04\n",
        "We wait for a few seconds for the reboot and then connect to the VM via SSH. We repeat the command to upgrade the version of the Ubuntu operating system:\n",
        "```bash\n",
        "$ sudo do-release-upgrade\n",
        "```\n",
        "Following the instructions, we accept the upgrade initially by choosing `'y'`. The next prompt asks us to press `[ENTER]`, and then another `'y'` to proceed with the process. Then, we will be asked for our permission to restart certain services that the system is running. We grant permission.\n",
        "![Allow the system to restart services](https://onedrive.live.com/embed?resid=23805F5DB37ABB76%212102&authkey=%21AEoQG9snCgmMS2Q&width=1251&height=233)\n",
        "We accept the upgrade of the LXD snap package to version `4.0`.\n",
        "![Upgrade LXD snap to version 4.0](https://onedrive.live.com/embed?resid=23805F5DB37ABB76%212103&authkey=%21AOXa1TPtYBxHEAA&width=1203&height=325)\n",
        "Similarly to previous steps, another prompt will ask us to remove installed packages that are no longer necessary. After accepting it, the system will prompt us to proceed with a restart and will start loading the latest version of the operating system.\n",
        "\n",
        "- *Note*: Installing the LXD snap requires a network with a public IPv4 address. Therefore, if you want to install it on both VMs in your team, you should disconnect the public IP from the master after its upgrade, connect it to the worker, and after the worker is also upgraded, return it to the master.\n",
        "\n",
        "#### 1.2 Ubuntu 20.04 -> Ubuntu 22.04\n",
        "We wait for a few seconds for the reboot and then connect to the VM with SSH. We repeat the command to upgrade the version of the Ubuntu operating system:\n",
        "```bash\n",
        "$ sudo do-release-upgrade\n",
        "```\n",
        "Just like the previous times, we accept the upgrade initially by choosing `'y'`. The next prompt asks us to press `[ENTER]`, and then another `'y'` to proceed with the process. We accept the removal of unnecessary software packages and restart as in the previous steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVipleZma-DY"
      },
      "source": [
        "### 2. Installation and configuration of Apache Spark over YARN on the small cluster of Virtual Machines\n",
        "**Note:** The following instructions assume that the actions described above in the same guide have been followed faithfully. Students are free to change the given pieces of code as per their own preferences in the configuration.\n",
        "\n",
        "#### 2.1 Java Installation\n",
        "We continue with the installation of Apache Hadoop on our new cluster. Initially, we need to install the Java version supported by the official repositories of our operating system on all nodes:\n",
        "```\n",
        "$ sudo apt install default-jdk -y\n",
        "```\n",
        "Successful installation can be verified with the following command:\n",
        "```\n",
        "$ java -version\n",
        "```\n",
        "#### 2.2 Installation of Hadoop and Apache Spark\n",
        "In order to install the latest versions of Apache Spark and Hadoop, we create the `~/opt` directory where we will store the executables of the latest versions of the two Apache projects.\n",
        "```bash\n",
        "$ mkdir ./opt\n",
        "$ mkdir ./opt/bin\n",
        "```\n",
        "Next, we download the compressed files from the official websites of both projects, extract their contents, move them to the `~/opt/bin` directory, and create links in the main  `~/opt` directory.\n",
        "```bash\n",
        "$ wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n",
        "$ tar -xvzf hadoop-3.3.6.tar.gz\n",
        "$ mv hadoop-3.3.6 ./opt/bin\n",
        "$ wget https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "$ tar -xvzf spark-3.5.0-bin-hadoop3.tgz\n",
        "$ mv ./spark-3.5.0-bin-hadoop3 ./opt/bin/\n",
        "$ cd ./opt\n",
        "$ ln -s ./bin/hadoop-3.3.6/ ./hadoop\n",
        "$ ln -s ./bin/spark-3.5.0-bin-hadoop3/ ./spark\n",
        "$ cd\n",
        "$ rm hadoop-3.3.6.tar.gz\n",
        "$ rm spark-3.5.0-bin-hadoop3.tgz\n",
        "$ mkdir ~/opt/data\n",
        "$ mkdir ~/opt/data/hadoop\n",
        "$ mkdir ~/opt/data/hdfs\n",
        "```\n",
        "\n",
        "#### 2.3 Configuration of environment variables\n",
        "To configure environment variables, you will initially edit the `~/.bashrc` file using your preferred text editor. This file is executed every time a terminal connection to your virtual machines is opened. You can add the following lines to it:\n",
        "```bash\n",
        "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64  #Value should match: dirname $(dirname $(readlink -f $(which java)))\n",
        "export HADOOP_HOME=/home/user/opt/hadoop\n",
        "export SPARK_HOME=/home/user/opt/spark\n",
        "export HADOOP_INSTALL=$HADOOP_HOME\n",
        "export HADOOP_MAPRED_HOME=$HADOOP_HOME\n",
        "export HADOOP_COMMON_HOME=$HADOOP_HOME\n",
        "export HADOOP_HDFS_HOME=$HADOOP_HOME\n",
        "export HADOOP_YARN_HOME=$HADOOP_HOME\n",
        "export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\n",
        "export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$SPARK_HOME/bin;\n",
        "export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\n",
        "export HADOOP_OPTS=\"-Djava.library.path=$HADOOP_HOME/lib/native\"\n",
        "export LD_LIBRARY_PATH=/home/ubuntu/opt/hadoop/lib/native:$LD_LIBRARY_PATH\n",
        "export PYSPARK_PYTHON=python3\n",
        "```\n",
        "Make sure to save the changes to the ~/.bashrc file after adding these lines.\n",
        "To update the environment, execute the following command to apply the changes from the script:\n",
        "```bash\n",
        "$ source ~/.bashrc\n",
        "```\n",
        "#### 2.4 Hadoop Distributed File System configuration\n",
        "The Hadoop configuration files are located in the `/home/user/opt/hadoop/etc/hadoop` directory. Since we've already set the `HADOOP_HOME` variable to point to the `/home/user/opt/hadoop` directory, we refer to the same path by writing: `$HADOOP_HOME/etc/hadoop/`. Below, we record the changes that need to be made:\n",
        "\n",
        "We start with `$HADOOP_HOME/etc/hadoop/hadoop-env.sh`, where the following line needs to be added:\n",
        "```bash\n",
        "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\n",
        "```\n",
        "\n",
        "We edit `$HADOOP_HOME/etc/hadoop/core-site.xml` so that it contains the following:\n",
        "```xml\n",
        "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>hadoop.tmp.dir</name>\n",
        "        <value>/home/user/opt/data/hadoop</value>\n",
        "        <description>Parent directory for other temporary directories.</description>\n",
        "    </property>\n",
        "    <property>\n",
        "        <name>fs.defaultFS </name>\n",
        "        <value>hdfs://okeanos-master:54310</value>\n",
        "        <description>The name of the default file system. </description>\n",
        "    </property>\n",
        "</configuration>\n",
        "```\n",
        "Επίσης, διαμορφώνουμε το αρχείο `$HADOOP_HOME/etc/hadoop/hdfs-site.xml` ώστε τα περιεχόμενά του να είναι τα ακόλουθα:\n",
        "```xml\n",
        "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>dfs.replication</name>\n",
        "        <value>1</value>\n",
        "        <description>Default block replication.</description>\n",
        "    </property>\n",
        "    <property>\n",
        "        <name>dfs.datanode.data.dir</name>\n",
        "        <value>/home/user/opt/data/hdfs</value>\n",
        "    </property>\n",
        "</configuration>\n",
        "```\n",
        "Finally, we create a new file, `$HADOOP_HOME/etc/hadoop/workers` and edit it to contain the next two lines:\n",
        " ```text\n",
        " okeanos-master\n",
        " okeanos-worker\n",
        " ```\n",
        "We register both VMs as workers, as the master will also act as one.\n",
        "\n",
        "\n",
        "#### 2.5 Starting and experimenting with HDFS\n",
        "We start HDFS using the following commands:\n",
        " ```bash\n",
        " $ $HADOOP_HOME/bin/hdfs namenode -format\n",
        " $ start-dfs.sh\n",
        " ```\n",
        "We confirm that the process was successful by accessing the web interface on the master's public IP using a browser: `http://83.212.xxx.xxx:9870`. We should see 2 available live nodes.\n",
        "\n",
        "#### 2.6 Hadoop YARN configuration\n",
        "We edit `$HADOOP_HOME/etc/hadoop/yarn-site.xml`, the contents of which need to be the following:\n",
        "\n",
        "```xml\n",
        "<?xml version=\"1.0\"?>\n",
        "<configuration>\n",
        "<!-- Site specific YARN configuration properties -->\n",
        "    <property>\n",
        "        <name>yarn.resourcemanager.hostname</name>\n",
        "        <value>okeanos-master</value>\n",
        "    </property>\n",
        "    <property>\n",
        "        <name>yarn.resourcemanager.webapp.address</name>\n",
        "        <!--Insert the public IP of your master machine here-->\n",
        "        <value>83.212.xxx.xxx:8088</value>\n",
        "    </property>\n",
        "    <property>\n",
        "        <name>yarn.nodemanager.resource.memory-mb</name>\n",
        "        <value>6144</value>\n",
        "    </property>\n",
        "    <property>\n",
        "        <name>yarn.scheduler.maximum-allocation-mb</name>\n",
        "        <value>6144</value>\n",
        "    </property>\n",
        "    <property>\n",
        "        <name>yarn.scheduler.minimum-allocation-mb</name>\n",
        "        <value>128</value>\n",
        "    </property>\n",
        "    <property>\n",
        "        <name>yarn.nodemanager.vmem-check-enabled</name>\n",
        "        <value>false</value>\n",
        "    </property>\n",
        "   <property>\n",
        "        <name>yarn.nodemanager.aux-services</name>\n",
        "        <value>mapreduce_shuffle,spark_shuffle</value>\n",
        "    </property>\n",
        "    <property>\n",
        "        <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>\n",
        "        <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n",
        "    </property>\n",
        "    <property>\n",
        "        <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>\n",
        "        <value>org.apache.spark.network.yarn.YarnShuffleService</value>\n",
        "    </property>\n",
        "    <property>\n",
        "        <name>yarn.nodemanager.aux-services.spark_shuffle.classpath</name>\n",
        "        <value>/home/user/opt/spark/yarn/*</value>\n",
        "    </property>\n",
        "</configuration>\n",
        "\n",
        "```\n",
        "We start YARN using the following command:\n",
        " ```bash\n",
        " $ start-yarn.sh\n",
        " ```\n",
        "We confirm that the process was successful by accessing the web interface on the master's public IP using a browser: `http://83.212.xxx.xxx::8088/cluster`. We should see 2 available live nodes.\n",
        "\n",
        "\n",
        "#### 2.7 Spark configuration\n",
        "We create the file `$SPARK_HOME/conf/spark-defaults.conf`, where we define the basic properties of our job execution environment, and we copy the following contents:\n",
        "```text\n",
        "spark.eventLog.enabled          true\n",
        "spark.eventLog.dir              hdfs://okeanos-master:54310/spark.eventLog\n",
        "spark.history.fs.logDirectory   hdfs://okeanos-master:54310/spark.eventLog\n",
        "spark.master                    yarn\n",
        "spark.submit.deployMode         client\n",
        "spark.driver.memory             1g\n",
        "spark.executor.memory           1g\n",
        "spark.executor.cores            1\n",
        "```\n",
        "\n",
        "We create the directory in HDFS where historical data of our jobs will be stored and start the Spark history server:\n",
        "```bash\n",
        "$ hadoop fs -mkdir /spark.eventLog\n",
        "$ $SPARK_HOME/sbin/start-history-server.sh\n",
        "```\n",
        "\n",
        "We confirm that the process has been successful by accessing the web interface at the public IP of the master: `http://83.212.xxx.xxx:18080`.\n",
        "\n",
        "#### 2.8 Εκτέλεση παραδείγματος εφαρμογής Spark για επιβεβαίωση ορθότητας\n",
        "We are ready to use our fresh infrastructure. Execute:\n",
        "```bash\n",
        "$ spark-submit --class org.apache.spark.examples.SparkPi ~/opt/spark/examples/jars/spark-examples_2.12-3.5.0.jar 100\n",
        "```\n",
        "You should be able to monitor the progress of the work in both the YARN web application (`http://83.212.xxx.xxx:8088`) and the history server (`http://83.212.xxx.xxx:18080`) after it is completed.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Homework!\n",
        "\n",
        "*   Find the differences between client and cluster mode execution in YARN. What will you use for development and what in production?\n",
        "*   Find out how you can change the number of Spark executors by running `spark-submit` from the command line. Experiment with different numbers of executors. What do you observe (and why)?\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SexRCrKCg8MD"
      }
    }
  ]
}